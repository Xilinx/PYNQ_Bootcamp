{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5158dd12",
   "metadata": {},
   "source": [
    "# Pose Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae990e",
   "metadata": {},
   "source": [
    "In this training, we are going to try to detect the position of a person's arms, hands, legs, and feet, from an input image. This is called pose detection. We use a model called the \"OpenPose\" model, which can detect up to 10 people in an image. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1592637",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Let's start by loading in all the functionality we are going to need later. It is considered good style to put all of your imports at the beginning, so that's what we do. The comments explain what we use each import for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e2c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                         # used to execute bash commands to download and uncompress stuff\n",
    "import cv2                        # OpenCV image processing library\n",
    "import numpy as np                # For math\n",
    "import matplotlib.pyplot as plt   # For plotting stuff\n",
    "from pynq_dpu import DpuOverlay   # Overlay for the FPGA\n",
    "import IPython.display            # For displaying a live feed of video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3612f8",
   "metadata": {},
   "source": [
    "\n",
    "## FPGA Overlays\n",
    "\n",
    "We run the model on specialized hardware that will allow it to execute faster than on a regular processor. The fastest option would be if we could build custom hardware -- a specialized electronic circuit -- that performs exactly the computations we need. However, soldering your own circuits is hard and takes a lot of time. Also, once such a specialized circuit is built, it can only be used for one thing -- the specific thing it was built for.\n",
    "\n",
    "FPGAs are the next best thing. They are like circuits, but instead of having to solder them physically, the different electrical connections can be \"programmed\". FPGAs consist of a large number of gates that we can use to route electricity (which we use to encode data) different ways. This is different from \"normal\" processor programming: Regular processors read a stream of instructions (our program) and execute those instructions. On FPGAs, we configure a circuit once ahead of time, and can then use it to perform the function it was designed to do multiple times very quickly. The KRIA SoM comes with an FPGA. \n",
    "\n",
    "Engineering these FPGA circuits is hard. For this reason, we will use a circuit somebody else already built for us! This is called an \"overlay\". The circuit we will use allows us to do lightning-fast data processing. This is exactly what we need to perform machine learning inference. Let us first load this circuit. By running the following step, you will configure the electronic circuit on your FPGA as a data processing unit (DPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d091736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\ntry {\nrequire(['notebook/js/codecell'], function(codecell) {\n  codecell.CodeCell.options_default.highlight_modes[\n      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n      Jupyter.notebook.get_cells().map(function(cell){\n          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n  });\n});\n} catch (e) {};\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\ntry {\nrequire(['notebook/js/codecell'], function(codecell) {\n  codecell.CodeCell.options_default.highlight_modes[\n      'magic_text/x-csrc'] = {'reg':[/^%%pybind11/]};\n  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n      Jupyter.notebook.get_cells().map(function(cell){\n          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n  });\n});\n} catch (e) {};\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "overlay = DpuOverlay(\"dpu.bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c282993",
   "metadata": {},
   "source": [
    "**Note:** If you run into a \"Device or resource busy\" error, quit all your Jupyter notebook kernels and restart this one. Then, try again. You can quit notebook kernels by clicking Kernel > Shutdown and restart by clicking Kernel > Restart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7afa88",
   "metadata": {},
   "source": [
    "## AI Model Download\n",
    "\n",
    "How ML models work internally is not super important to us as users of the models. What we need to know is that these models take an input matrix, perform some calculations, and produce an output matrix of potentially a different dimension.\n",
    "\n",
    "What the meaning of the input and output matrices is depends on the model. In our model, the input matrix represents an image: Each cell in the matrix can be thought of as a pixel -- a red/green/blue color value at that coordinate in the image. The model outputs many matrices, which can be interpreted as probabilities that a limb is present in a certain pixel.\n",
    "\n",
    "When models are built, they are \"trained\" on a large set of input data. This simply means that the coefficients of the calculations are optimized. In order to run the model on our FPGA overlay, some additional modifications are required. Someone has already done this for us, so we only have to download the model.\n",
    "\n",
    "We download the model from the URL listed in the Model Zoo YAML file. Make sure to use the v2.5 branch; v2.0 will crash the notebook. The model can be used straight from the download without any compilation.\n",
    "\n",
    "This model expects input of dimensions 368x368."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "902f2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_filename = \"openpose_pruned_0_3.tar.gz\"\n",
    "extracted_model_path = \"openpose_pruned_0_3/openpose_pruned_0_3.xmodel\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3125abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following will download the model if it is not already present in the folder:\n",
    "# model_download_url = \"https://www.xilinx.com/bin/public/openDownload?filename=openpose_pruned_0_3-zcu102_zcu104_kv260-r2.5.0.tar.gz\"\n",
    "# os.system(\"wget -nv -O \\\"{}\\\" \\\"{}\\\"\".format(archive_filename, model_download_url))\n",
    "# os.system(\"tar -xvf \\\"{}\\\"\".format(archive_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df29f737",
   "metadata": {},
   "source": [
    "## Load Model Onto Overlay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce82992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay.load_model(extracted_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe01fc0",
   "metadata": {},
   "source": [
    "## Load an Image\n",
    "\n",
    "To test the model, let's load an image of a crowd and see if the model can detect the pose of people in it.\n",
    "\n",
    "The model works on matrices of size 800 x 1000. So first, we need to get our input into the right format. We load a JPEG, decompress it into its raw pixel values, remove the color information and put each pixel value on a 0 - 1 scale. Since the input image also does not exactly match the 800 x 1000 format expected by the model, we crop it slightly. We use the OpenCV library which provides functions to do this necessary processing on the image.\n",
    "\n",
    "_Image: Sydney International Tennis ATP, licensed CC-BY-SA 2.0._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38860a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The test image should have been shipped with this notebook; if not, download it like so:\n",
    "# img_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Sydney_International_Tennis_ATP_%2833040179108%29.jpg/640px-Sydney_International_Tennis_ATP_%2833040179108%29.jpg\"\n",
    "# os.system(\"wget -nv -O test_image.jpeg {}\".format(img_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be57f51",
   "metadata": {},
   "source": [
    "The image shows with distorted colors because CV2 reads as BGR but matplotlib expects RGB. We don't convert because the OpenPose model expects BGR too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f0a619",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"test_image.jpeg\")\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Need BGR for OpenPose\n",
    "height, width = img.shape[0], img.shape[1]\n",
    "aspect_ratio = width / height\n",
    "width, height, aspect_ratio, plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f148ed",
   "metadata": {},
   "source": [
    "## Prepare the DPU\n",
    "\n",
    "Different models have different input and output sizes. Let us see what the expected dimensions are for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6244de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpu = overlay.runner\n",
    "\n",
    "inputTensors = dpu.get_input_tensors()\n",
    "outputTensors = dpu.get_output_tensors()\n",
    "\n",
    "shapeIn = tuple(inputTensors[0].dims)\n",
    "shapeOut = tuple(outputTensors[0].dims)\n",
    "outputSize = int(outputTensors[0].get_data_size() / shapeIn[0])\n",
    "shapeIn, shapeOut, outputSize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa8675f",
   "metadata": {},
   "source": [
    "## Adjust the Image\n",
    "\n",
    "As we can see, we need an image that has different dimensions than our image we read in previously. To address this, let's pad the image on top and bottom a little:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a6947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpad = 0\n",
    "ypad = 20\n",
    "canvas = 255*np.ones((height+2*ypad, width+2*xpad, 3), dtype=img.dtype)\n",
    "canvas[ypad:height+ypad, xpad:width+xpad, :] = img\n",
    "img = canvas\n",
    "plt.imshow(canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e32ed",
   "metadata": {},
   "source": [
    "Now, let's resize and crop it to get the corrected dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d8529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_height = shapeIn[1]\n",
    "img_resized = cv2.resize(img, \n",
    "                         (int(aspect_ratio*new_height),   # new width; maintain aspect ratio\n",
    "                          new_height))                    # new height\n",
    "img_cropped = img_resized[0:shapeIn[1],0:shapeIn[2],:].reshape((shapeIn[1],shapeIn[2],3))\n",
    "img_cropped = img_cropped.astype(np.float32) / 255.0  # Make it floats\n",
    "plt.imshow(img_cropped), img_cropped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25649366",
   "metadata": {},
   "source": [
    "## Run the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce533934",
   "metadata": {},
   "source": [
    "Now that the model is loaded, the DPU is ready, and the image has been properly prepared, there's nothing stopping us from running the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c14ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare an array of zeros with the correct output dimensions; the DPU will fill in the results in here, but we \n",
    "# have to prepare some space for it.\n",
    "output_data = [np.zeros(shapeOut, dtype=np.float32, order=\"C\")]\n",
    "\n",
    "# We will be putting our image into this array:\n",
    "input_data = [np.zeros(shapeIn, dtype=np.float32, order=\"C\")]\n",
    "# ... but first, we must adjust the shape of the matrix slightly ...\n",
    "processed_image = img_cropped\n",
    "processed_image = processed_image.reshape(shapeIn).astype(np.float32, order=\"C\")\n",
    "input_data[0] = processed_image\n",
    "\n",
    "# Now, we can send our input to the model ...\n",
    "job_id = dpu.execute_async(input_data, output_data)\n",
    "# ... and wait for it to be done doing its calculation\n",
    "dpu.wait(job_id)\n",
    "\n",
    "# Now, the results are in output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b6783",
   "metadata": {},
   "source": [
    "But did it work? Let's plot the output of the model!\n",
    "\n",
    "The model will output several matrices. One of those is a heat map that gives the probabilities that a limb is present at a certain coordinate. A value close to 1 will indicate that the model thinks there is a limb in that model. For now, we will just take the max of these matrices to get an iteresting output. This causes us to lose some information, i.e. which limb the model detected, but it gives us something interesting to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f1cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_cropped)\n",
    "heatmap = np.squeeze(np.max(output_data[0][0][:][:][:],axis=2))\n",
    "heatmap = cv2.resize(heatmap, (shapeIn[2], shapeIn[1]))\n",
    "plt.imshow(heatmap, cmap='hot', interpolation='nearest', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce499d",
   "metadata": {},
   "source": [
    "Looking good! For some reason, the model does not seem to detect the right arm, but it detected the other limbs accurately. AI is not perfect -- you are going to see that in your own experiments too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403a135",
   "metadata": {},
   "source": [
    "## Put it all in functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16a9463",
   "metadata": {},
   "source": [
    "Next, we will try to apply the model to a live video feed. For this, let us first take all the stuff we programmed above, and put it into functions so we can reuse it without having to repeat ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "160e9ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # Need BGR for OpenPose\n",
    "\n",
    "    # Get dimensions\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    aspect_ratio = width / height\n",
    "    width, height, aspect_ratio, plt.imshow(img)\n",
    "    \n",
    "    # Padding\n",
    "    xpad = 0\n",
    "    ypad = 20\n",
    "    canvas = 255*np.ones((height+2*ypad, width+2*xpad, 3), dtype=img.dtype)\n",
    "    canvas[ypad:height+ypad, xpad:width+xpad, :] = img\n",
    "    img = canvas\n",
    "    \n",
    "    # Crop\n",
    "    new_width = shapeIn[1]\n",
    "    img_resized = cv2.resize(img, (int(aspect_ratio*new_width), new_width))\n",
    "    img_cropped = img_resized[0:shapeIn[1],0:shapeIn[2],:].reshape((shapeIn[1],shapeIn[2],3))\n",
    "    img_cropped = img_cropped.astype(np.float32) / 255.0  # Make it floats\n",
    "    \n",
    "    return img_cropped\n",
    "\n",
    "def run_model(img_cropped):\n",
    "    processed_image = img_cropped\n",
    "    processed_image = processed_image.reshape(shapeIn).astype(np.float32, order=\"C\")\n",
    "    output_data = [np.zeros(shapeOut, dtype=np.float32, order=\"C\")]\n",
    "    input_data = [np.zeros(shapeIn, dtype=np.float32, order=\"C\")]\n",
    "    input_data[0] = processed_image\n",
    "    job_id = dpu.execute_async(input_data, output_data)\n",
    "    dpu.wait(job_id)\n",
    "    return output_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71b45d0",
   "metadata": {},
   "source": [
    "## Getting a live video feed\n",
    "\n",
    "To test webcam: ffplay /dev/video0\n",
    "\n",
    "You may have to re-run the first code block if it can't open the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe77e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "capture = cv2.VideoCapture(0)  # Get a handle on the camera; if you have multiple cameras, you can use different indices\n",
    "capture.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # Disable buffering\n",
    "capture.set(3, shapeIn[1])   # set input width\n",
    "capture.set(4, shapeIn[2])   # set input height\n",
    "n_frames = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e66bd3",
   "metadata": {},
   "source": [
    "After initial setup above, let's do the same processing we did above for ten frames in a row. We will show each frame after the model has run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b4838",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_frames):\n",
    "    success, newimg = capture.read()\n",
    "    if not success:\n",
    "        print(\"Error reading webcam image.\")\n",
    "        break\n",
    "    img_cropped = preprocess_image(newimg)\n",
    "    output = run_model(img_cropped)\n",
    "    plt.imshow(img_cropped)\n",
    "    heatmap = np.squeeze(np.max(output[0][:][:][:],axis=2))\n",
    "    heatmap = cv2.resize(heatmap, (shapeIn[2], shapeIn[1]))\n",
    "    plt.imshow(heatmap, cmap='hot', interpolation='nearest', alpha=0.4)\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    IPython.display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f7565f",
   "metadata": {},
   "source": [
    "## Conclusion and Extra Challenge\n",
    "\n",
    "This looks pretty good! What uses for this model can you think of?\n",
    "\n",
    "If you have gotten this far with time left over, try to derive the X/Y coordinates of all limbs from the heatmap. This is harder than it seems! Hint: You will have to write code that detects local maxima, that is, find coordinates in the output matrix where all surrounding values are lower than the value you are looking at; this is a local maximum."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
