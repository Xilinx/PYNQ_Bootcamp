{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision\n",
    "\n",
    "### In this tutorial, we are going to learn how to take a photo with a webcam attached to our Pynq Board, display this image in our Jupyter Notebook, and then detect faces in the photo. For extra credit, we will use a neural net to predict the objects in a photo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Overlay\n",
    "Our PYNQ Board has a Field Programmable Gate Array (FPGA) on it that must be programmed before we begin using the board. We apply designs called overlays that we can design however we want. For the first part of this tutorial, we are going to use the pre-built base overlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynq.overlays.base import BaseOverlay\n",
    "from pynq.lib.video import *\n",
    "base = BaseOverlay(\"base.bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import Stuff We Need\n",
    "Here we tell the board what libraries we want to use in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Initialize Webcam\n",
    "After plugging in our USB webcam, we must tell the board what size images it is going to be recording and sending to the board. We also need to create a python object that will store the images we read from the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create object\n",
    "\n",
    "#set input width and height\n",
    "\n",
    "#check to ensure the webcam is open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Take Photo\n",
    "Once we have our board setup and webcam working, it is time to use it! using videoIn.read(), we can read what the webcam is looking at at any given moment. This function has two return values. The first tells us if we successfully read an image or not. True means we did, false means there was a problem. The second return value is a frame object, which is the actual image we read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read frame\n",
    "\n",
    "#if there was an error, tell us!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Display Photo\n",
    "Using matplotlib imported earlier, we can display our image right in our Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#display input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Look for Faces\n",
    "Now that we can take a photo, let's have our board look for faces in our photo. We are going to use something called a Haar Cascade Classifier. This is a model that has been training to understand what a face looks like. It has already been built and trained, so we simply need to load it and pass our image to it. Below is what the model is looking for when it tries to find faces, can you trick it?\n",
    "![HaarClassifier](haar.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our own instance of the classifier\n",
    "\n",
    "\n",
    "#our classifier prefers to work on a gray image\n",
    "\n",
    "\n",
    "#draw a rectangle around any faces we find\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Show our image again with the face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Look for Eyes\n",
    "The board also have another classifier for detecting features that look like eyes. We will do the same thing we did with the face classifier for the eye classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our own instance of the classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Show image with eye and face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Counting Faces\n",
    "The faces our program found are contained in an array. This stores the location of the faces within the image. If we want to count the faces found in the image, we can simply print the length of this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the number of faces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Guess the object with a neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Import More Things We Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! sudo pip3 install git+https://github.com/Xilinx/QNN-MO-PYNQ.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qnn\n",
    "from qnn import Dorefanet\n",
    "from qnn import utils\n",
    "import os, pickle, random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Take Another Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#read a frame from the camera\n",
    "\n",
    "#if there was an error, tell us!\n",
    "\n",
    "    \n",
    "#display the image\n",
    "\n",
    "#save the image just taken to memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Set Up the Object Classifier\n",
    "This classifier has been trained to recognize a wide variety of objects. It requires us to reconfigure the FPGA on the board (replacing the base overlay with a custom overlay that will help accelerate the classification). This is automatically done with the init_accelerator() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Dorefanet()\n",
    "classifier.init_accelerator()\n",
    "net = classifier.load_network(json_layer=\"/usr/local/lib/python3.6/dist-packages/qnn/params/dorefanet-layers.json\")\n",
    "\n",
    "conv0_weights = np.load('/usr/local/lib/python3.6/dist-packages/qnn/params/dorefanet-conv0.npy', encoding=\"latin1\").item()\n",
    "fc_weights = np.load('/usr/local/lib/python3.6/dist-packages/qnn/params/dorefanet-fc-normalized.npy', encoding='latin1').item()\n",
    "\n",
    "with open(\"/home/xilinx/jupyter_notebooks/qnn/imagenet-classes.pkl\", 'rb') as f:\n",
    "    classes = pickle.load(f)\n",
    "    names = dict((k, classes[k][1].split(',')[0]) for k in classes.keys())\n",
    "    synsets = dict((classes[k][0], classes[k][1].split(',')[0]) for k in classes.keys())\n",
    "    \n",
    "conv0_W = conv0_weights['conv0/W']\n",
    "conv0_T = conv0_weights['conv0/T']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Run the Image through the Classifier\n",
    "Once our classifier is set up, we input our image to the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, img_class = classifier.load_image(\"photo.jpg\")\n",
    "\n",
    "# 1st convolutional layer execution, having as input the image and the trained parameters (weights)\n",
    "conv0 = utils.conv_layer(img, conv0_W, stride=4)\n",
    "# The result in then quantized to 2 bits representation for the subsequent HW offload\n",
    "conv0 = utils.threshold(conv0, conv0_T)\n",
    "\n",
    "\n",
    "# Compute offloaded convolutional layers\n",
    "out_dim = net['merge4']['output_dim']\n",
    "out_ch = net['merge4']['output_channels']\n",
    "\n",
    "conv_output = classifier.get_accel_buffer(out_ch, out_dim);\n",
    "conv_input = classifier.prepare_buffer(conv0)\n",
    "\n",
    "classifier.inference(conv_input, conv_output)\n",
    "\n",
    "conv_output = classifier.postprocess_buffer(conv_output)\n",
    "\n",
    "\n",
    "# Normalize results\n",
    "fc_input = conv_output / np.max(conv_output)\n",
    "\n",
    "# FC Layer 0\n",
    "fc0_W = fc_weights['fc0/Wn']\n",
    "fc0_b = fc_weights['fc0/bn']\n",
    "\n",
    "fc0_out = utils.fully_connected(fc_input, fc0_W, fc0_b)\n",
    "fc0_out = utils.qrelu(fc0_out)\n",
    "fc0_out = utils.quantize(fc0_out, 2)\n",
    "\n",
    "# FC Layer 1\n",
    "fc1_W = fc_weights['fc1/Wn']\n",
    "fc1_b = fc_weights['fc1/bn']\n",
    "\n",
    "fc1_out = utils.fully_connected(fc0_out, fc1_W, fc1_b)\n",
    "fc1_out = utils.qrelu(fc1_out)\n",
    "\n",
    "# FC Layer 2\n",
    "fct_W = fc_weights['fct/W']\n",
    "fct_b = np.zeros((fct_W.shape[1], ))\n",
    "\n",
    "fct_out = utils.fully_connected(fc1_out, fct_W, fct_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Use Softmax to Estimate the Probability of the Top 5 Most Likely Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "out = utils.softmax(fct_out)\n",
    "\n",
    "# Top-5 results\n",
    "topn =  utils.get_topn_indexes(out, 5)  \n",
    "for k in topn: print(\"class:{0:>20}\\tprobability:{1:>8.2%}\".format(names[k].lower(), out[k]))\n",
    "\n",
    "#display results\n",
    "x_pos = np.arange(len(topn))\n",
    "plt.barh(x_pos, out[topn], height=0.4, color='g', zorder=3)\n",
    "plt.yticks(x_pos, [names[k] for k in topn])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlim([0,1])\n",
    "plt.grid(zorder=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
