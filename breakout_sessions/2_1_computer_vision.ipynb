{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision\n",
    "\n",
    "### In this tutorial, we are going to learn how to take a photo with a webcam attached to our Pynq Board, display this image in our Jupyter Notebook, and then detect faces in the photo. For extra credit, we will use a neural net to predict the objects in a photo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Overlay\n",
    "Our PYNQ Board has a Field Programmable Gate Array (FPGA) on it that must be programmed before we begin using the board. We apply designs called overlays that we can design however we want. For the first part of this tutorial, we are going to use the pre-built base overlay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynq.overlays.base import BaseOverlay\n",
    "from pynq.lib.video import *\n",
    "base = BaseOverlay(\"base.bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import Libraries\n",
    "Here we tell the board what libraries we want to use in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Initialize Webcam\n",
    "After plugging in our USB webcam, we must tell the board what size images it is going to be recording and sending to the board. We also need to create a python object that will store the images we read from the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create object\n",
    "videoIn = cv2.VideoCapture(0)\n",
    "\n",
    "#set input width and height\n",
    "input_frame_width = 640\n",
    "input_frame_height = 480\n",
    "videoIn.set(cv2.CAP_PROP_FRAME_WIDTH, input_frame_width);\n",
    "videoIn.set(cv2.CAP_PROP_FRAME_HEIGHT, input_frame_height);\n",
    "\n",
    "#check to ensure the webcam is open\n",
    "if(videoIn.isOpened()):\n",
    "    print('camera is ready')\n",
    "else:\n",
    "    print('error starting camera, run this cell again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Take Photo\n",
    "Once we have our board setup and webcam working, it is time to use it! using videoIn.read(), we can read what the webcam is looking at at any given moment. This function has two return values. The first tells us if we successfully read an image or not. True means we did, false means there was a problem. The second return value is a frame object, which is the actual image we read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read frame\n",
    "success, frame = videoIn.read()\n",
    "#if there was an error, tell us!\n",
    "if (success != True):\n",
    "    print(\"Video Read Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Display Photo\n",
    "Using matplotlib imported earlier, we can display our image right in our Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display input\n",
    "plt.imshow(frame[:,:,[2,1,0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Look for Faces\n",
    "Now that we can take a photo, let's have our board look for faces in our photo. We are going to use something called a Haar Cascade Classifier. This is a model that has been training to understand what a face looks like. It has already been built and trained, so we simply need to load it and pass our image to it. Below is what the model is looking for when it tries to find faces, can you trick it?\n",
    "![HaarClassifier](data/haar.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our own instance of the classifier\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    '/home/xilinx/jupyter_notebooks/base/video/data/'\n",
    "    'haarcascade_frontalface_default.xml')\n",
    "\n",
    "#our classifier prefers to work on a gray image\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "#draw a rectangle around any faces we find\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = frame[y:y+h, x:x+w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Show our image again with the face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(frame[:,:,[2,1,0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Look for Eyes\n",
    "The board also have another classifier for detecting features that look like eyes. We will do the same thing we did with the face classifier for the eye classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our own instance of the classifier\n",
    "eye_cascade = cv2.CascadeClassifier(\n",
    "    '/home/xilinx/jupyter_notebooks/base/video/data/'\n",
    "    'haarcascade_eye.xml')\n",
    "\n",
    "eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "for (ex,ey,ew,eh) in eyes:\n",
    "    cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Show image with eye and face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(frame[:,:,[2,1,0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Counting Faces\n",
    "The faces our program found are contained in an array. This stores the location of the faces within the image. If we want to count the faces found in the image, we can simply print the length of this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(faces))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Guess the object with a neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Import a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! sudo pip3 install git+https://github.com/Xilinx/QNN-MO-PYNQ.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qnn\n",
    "from qnn import Dorefanet\n",
    "from qnn import utils\n",
    "import os, pickle, random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Take Another Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success, frame2 = videoIn.read()\n",
    "\n",
    "#if there was an error, tell us!\n",
    "if (success != True):\n",
    "    print(\"Video Read Error\")\n",
    "    \n",
    "#display the image\n",
    "plt.imshow(frame2[:,:,[2,1,0]])\n",
    "plt.show()   \n",
    "\n",
    "#save the image just taken to memory\n",
    "cv2.imwrite(\"photo.jpg\", frame2)\n",
    "\n",
    "videoIn.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Set Up the Object Classifier\n",
    "This classifier has been trained to recognize a wide variety of objects. It requires us to reconfigure the FPGA on the board (replacing the base overlay with a custom overlay that will help accelerate the classification). This is automatically done with the init_accelerator() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Dorefanet()\n",
    "classifier.init_accelerator()\n",
    "net = classifier.load_network(json_layer=\"/usr/local/lib/python3.6/dist-packages/qnn/params/dorefanet-layers.json\")\n",
    "\n",
    "conv0_weights = np.load('/usr/local/lib/python3.6/dist-packages/qnn/params/dorefanet-conv0.npy', encoding=\"latin1\").item()\n",
    "fc_weights = np.load('/usr/local/lib/python3.6/dist-packages/qnn/params/dorefanet-fc-normalized.npy', encoding='latin1').item()\n",
    "\n",
    "with open(\"/home/xilinx/jupyter_notebooks/qnn/imagenet-classes.pkl\", 'rb') as f:\n",
    "    classes = pickle.load(f)\n",
    "    names = dict((k, classes[k][1].split(',')[0]) for k in classes.keys())\n",
    "    synsets = dict((classes[k][0], classes[k][1].split(',')[0]) for k in classes.keys())\n",
    "    \n",
    "conv0_W = conv0_weights['conv0/W']\n",
    "conv0_T = conv0_weights['conv0/T']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Run the Image through the Classifier\n",
    "Once our classifier is set up, we input our image to the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, img_class = classifier.load_image(\"photo.jpg\")\n",
    "\n",
    "# 1st convolutional layer execution, having as input the image and the trained parameters (weights)\n",
    "conv0 = utils.conv_layer(img, conv0_W, stride=4)\n",
    "# The result in then quantized to 2 bits representation for the subsequent HW offload\n",
    "conv0 = utils.threshold(conv0, conv0_T)\n",
    "\n",
    "\n",
    "# Compute offloaded convolutional layers\n",
    "out_dim = net['merge4']['output_dim']\n",
    "out_ch = net['merge4']['output_channels']\n",
    "\n",
    "conv_output = classifier.get_accel_buffer(out_ch, out_dim);\n",
    "conv_input = classifier.prepare_buffer(conv0)\n",
    "\n",
    "classifier.inference(conv_input, conv_output)\n",
    "\n",
    "conv_output = classifier.postprocess_buffer(conv_output)\n",
    "\n",
    "\n",
    "# Normalize results\n",
    "fc_input = conv_output / np.max(conv_output)\n",
    "\n",
    "# FC Layer 0\n",
    "fc0_W = fc_weights['fc0/Wn']\n",
    "fc0_b = fc_weights['fc0/bn']\n",
    "\n",
    "fc0_out = utils.fully_connected(fc_input, fc0_W, fc0_b)\n",
    "fc0_out = utils.qrelu(fc0_out)\n",
    "fc0_out = utils.quantize(fc0_out, 2)\n",
    "\n",
    "# FC Layer 1\n",
    "fc1_W = fc_weights['fc1/Wn']\n",
    "fc1_b = fc_weights['fc1/bn']\n",
    "\n",
    "fc1_out = utils.fully_connected(fc0_out, fc1_W, fc1_b)\n",
    "fc1_out = utils.qrelu(fc1_out)\n",
    "\n",
    "# FC Layer 2\n",
    "fct_W = fc_weights['fct/W']\n",
    "fct_b = np.zeros((fct_W.shape[1], ))\n",
    "\n",
    "fct_out = utils.fully_connected(fc1_out, fct_W, fct_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Use Softmax to Estimate the Probability of the Top 5 Most Likely Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax\n",
    "out = utils.softmax(fct_out)\n",
    "\n",
    "# Top-5 results\n",
    "topn =  utils.get_topn_indexes(out, 5)  \n",
    "for k in topn: print(\"class:{0:>20}\\tprobability:{1:>8.2%}\".format(names[k].lower(), out[k]))\n",
    "\n",
    "#display results\n",
    "x_pos = np.arange(len(topn))\n",
    "plt.barh(x_pos, out[topn], height=0.4, color='g', zorder=3)\n",
    "plt.yticks(x_pos, [names[k] for k in topn])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlim([0,1])\n",
    "plt.grid(zorder=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Gesture Sensor\n",
    "\n",
    "Now we will explore the Grove Gesture Sensor and use it to take pictures with our webcam.\n",
    "\n",
    "[Grove gesture sensor](http://wiki.seeedstudio.com/Grove-Gesture_v1.0/) on the board.\n",
    "The gesture sensor can detect 10 gestures as follows:\n",
    "\n",
    "| Value the Sensor Returns | Gesture            |\n",
    "|--------------------------|--------------------|\n",
    "| 0                        | No detection       |\n",
    "| 1                        | forward            |\n",
    "| 2                        | backward           |\n",
    "| 3                        | right              |\n",
    "| 4                        | left               |\n",
    "| 5                        | up                 |\n",
    "| 6                        | down               |\n",
    "| 7                        | clockwise          |\n",
    "| 8                        | counter-clockwise  |\n",
    "| 9                        | wave               |\n",
    "\n",
    "\n",
    "For this notebook, a PYNQ Arduino shield is also required.\n",
    "The grove gesture sensor is attached to the I2C interface on the shield. \n",
    "This grove sensor should also work with PMOD interfaces on the board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynq.lib.arduino import Grove_Gesture\n",
    "from pynq.lib.arduino import ARDUINO_GROVE_I2C\n",
    "\n",
    "sensor = Grove_Gesture(base.ARDUINO, ARDUINO_GROVE_I2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Set speed\n",
    "There are currently 2 modes available for users to use: `far` and `near`.\n",
    "The corresponding fps are 120 and 240, respectively.\n",
    "For more information, please refer to [Grove gesture sensor](http://wiki.seeedstudio.com/Grove-Gesture_v1.0/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor.set_speed(240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Read gestures\n",
    "The following code will read 10 gestures within 30 seconds. \n",
    "Try to change your gesture in front of the sensor and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "for i in range(10):\n",
    "    print(sensor.read_gesture())\n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
